---
title: "The tortoise and the hare"
author: "Ian Hussey^[Ghent University. Email: ian.hussey@ugent.be]"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

# set defaults
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

```

```{r}

# dependencies
library(tidyverse)
library(timesavers)
library(pwr)

# disable scientific notation
options(scipen = 999) 

# knitr output for html (comment out if knitting to pdf)
options(knitr.table.format = "html")

```

# Simulation

```{r}

# These could be participants, dollars, etc
units_of_resources <- 10000

# function to calculate diagnosticity of a p value
p_diagnosticity <- function(alpha = 0.05, power = 0.80, baserate_of_true_hypotheses = 0.50) {
  
  require(dplyr)
  
  # true and false positives
  h1_true <- rep(c(rep(0, times = (1-power)*100), 
                   rep(1, times = power*100)), 
                 times = baserate_of_true_hypotheses*100)
  
  # true and false negatives
  h0_true <- rep(c(rep(0, times = (1-alpha)*100), 
                   rep(1, times = alpha*100)), 
                 times = (1-baserate_of_true_hypotheses)*100)
  
  # all findings
  all_tests <- append(h1_true, h0_true)
  
  # probability of finding significant results, agnostic to their truth corrispondance
  positive_result_rate <- round(length(h1_true[h1_true == 1]) / length(all_tests), 4)

  # PPV = TP/(TP + FP)
  # nb false discovery rate = 1 - PPV
  positive_predictive_value <- round(length(h1_true[h1_true == 1]) / length(all_tests[all_tests == 1]), 4)
  
  # FOR = FN/(TN + FN). 
  # NB negative predictive value = 1 - FOR 
  false_omission_rate       <- round(length(h1_true[h1_true == 0]) / length(all_tests[all_tests == 0]), 4)
  
  return(data.frame(positive_result_rate      = positive_result_rate,
                    positive_predictive_value = positive_predictive_value, 
                    false_omission_rate       = false_omission_rate))
  
} 

# tortoise simulation
results_list_hare <- list()

n_per_cell <- 25
alpha <- 0.05

for (effect_size in seq(from = 0.2, to = 0.8, by = 0.05)) {
  for (baserate_of_true_hypotheses in seq(from = 5, to = 95, by = 5)) {
    
    power_calc <- pwr.t.test(n           = n_per_cell,
                             d           = effect_size,
                             sig.level   = alpha,
                             power       = NULL,
                             alternative = "two.sided")
    
    result_df <- data.frame(total_n = as.numeric(power_calc$n*2),
                            effect_size = power_calc$d,
                            alpha   = power_calc$sig.level,
                            power   = round(power_calc$power, 2),
                            baserate_of_true_hypotheses = baserate_of_true_hypotheses/100)
    
    diagnosticity_results <- p_diagnosticity(alpha = result_df$alpha,
                                             power = result_df$power,
                                             baserate_of_true_hypotheses = result_df$baserate_of_true_hypotheses)
    
    results <- bind_cols(result_df, diagnosticity_results)
    
    results_list_hare[[paste(alpha, effect_size, n_per_cell, baserate_of_true_hypotheses, sep = "_")]] <- results
    
  }
}

discoveries_hare <- bind_rows(results_list_hare) %>%
  mutate(strategy = "hare",
         n_studies = units_of_resources/total_n,
         n_positive_results = positive_result_rate * n_studies,
         n_true_discoveries = positive_predictive_value * n_studies,
         n_false_discoveries = (1 - positive_predictive_value) * n_studies,
         n_false_omissions = false_omission_rate * n_studies)


# hare simulation
results_list_tortoise <- list()

alpha <- 0.05
power <- 0.80

for (effect_size in seq(from = 0.2, to = 0.8, by = 0.05)) {
  for (baserate_of_true_hypotheses in seq(from = 5, to = 95, by = 5)) {
    
    power_calc <- pwr.t.test(n           = NULL,
                             d           = effect_size,
                             sig.level   = alpha,
                             power       = power,
                             alternative = "two.sided")
    
    result_df <- data.frame(total_n     = power_calc$n*2,
                            effect_size = power_calc$d,
                            alpha       = power_calc$sig.level,
                            power       = round(power_calc$power, 2),
                            baserate_of_true_hypotheses = baserate_of_true_hypotheses/100)
    
    diagnosticity_results <- p_diagnosticity(alpha = result_df$alpha,
                                             power = result_df$power,
                                             baserate_of_true_hypotheses = result_df$baserate_of_true_hypotheses)
    
    results <- bind_cols(result_df, diagnosticity_results)
    
    results_list_tortoise[[paste(alpha, effect_size, n_per_cell, baserate_of_true_hypotheses, sep = "_")]] <- results
    
  }
}

discoveries_tortoise <- bind_rows(results_list_tortoise) %>%
  mutate(strategy = "tortoise",
         n_studies = units_of_resources/total_n,
         n_positive_results = positive_result_rate * n_studies,
         n_true_discoveries = positive_predictive_value * n_studies,
         n_false_discoveries = (1 - positive_predictive_value) * n_studies,
         n_false_omissions = false_omission_rate * n_studies)

discoveries <- bind_rows(discoveries_hare, discoveries_tortoise)

```

# Analysis workflow

```{r}

workflow <- function(data){
  
  require(timesavers)
  require(tidyverse)
  
  summary_stats <- 
    data %>%
    dplyr::group_by(strategy) %>%
    dplyr::summarize(median = median(dv),
                     mad = mad(dv)) %>%
    timesavers::round_df(2)
  
  wilcox_test_results <-
    wilcox.test(dv ~ strategy,
                data = data) %>%
    broom::tidy()
  
  probability_of_superiority <- 
    timesavers::ruscios_A_boot(data = data,
                               variable = "dv",
                               group = "strategy",
                               value1 = "tortoise",
                               value2 = "hare") %>%
    dplyr::rename(prob_of_superiority = ruscios_A,
                  ci_lwr = ruscios_A_ci_lwr,
                  ci_upr = ruscios_A_ci_upr) %>%
    select(prob_of_superiority,
           ci_lwr,
           ci_upr)
  
  plot <- 
    ggplot(data, aes(dv, color = strategy, fill = strategy)) +
    geom_density(alpha = 0.5) +
    scale_color_viridis_d(begin = 0.3, end = 0.7) +
    scale_fill_viridis_d(begin = 0.3, end = 0.7) +
    theme_classic() 
  
  return(list(summary_stats = summary_stats,
              wilcox_test_results = wilcox_test_results,
              probability_of_superiority = probability_of_superiority,
              plot = plot))
  
}

```

# Activity and productivity

Add n of studies, n per study, n publishable sig results agnostic to their truth, etc.

## N studies

```{r}

discoveries %>%
  dplyr::mutate(dv = n_studies) %>%
  workflow()

```

## N positive tests

```{r}

discoveries %>%
  dplyr::mutate(dv = n_positive_results) %>%
  workflow()

```

# Discovery accuracy (truth uncovered vs mistakes intruced into the literature)

## True discovery rate

False discovery rate = 1 - FPR

```{r}

discoveries %>%
  dplyr::mutate(dv = positive_predictive_value) %>%
  workflow()

```

- Tortoise wins the race 

## Missed discovery rate

```{r}

discoveries %>%
  dplyr::mutate(dv = false_omission_rate) %>%
  workflow()

```

- Tortoise wins the race

# Discoveries (and errors) really made

## N true discoveries

per 10000 units if research resources (could be participants, dollars, etc).

```{r}

discoveries %>%
  dplyr::mutate(dv = n_true_discoveries) %>%
  workflow()

```

- Hare wins the race 
- Makes a median of 2.5 as many true discoveries

## N false discoveries

per 10000 units if research resources (could be participants, dollars, etc).

```{r}

discoveries %>%
  dplyr::mutate(dv = n_false_discoveries) %>%
  workflow()

```

- tortoise wins the race
- Makes a median of 7.5 as many false discoveries

## N missed discoveries 

```{r}

discoveries %>%
  dplyr::mutate(dv = n_false_omissions) %>%
  workflow()

```

- tortoise wins the race
- hare overlooks a median of 7.6 as many discoveries

## Add metric of signal to noise ratio?

e.g., the Jaccard index?

https://www.sciencedirect.com/science/article/pii/S2210832718301546

something that says how many true discoveries do you make for every false (and missed one)? One could even also apply a penality for false positives, as these zombine hypotheses tend to live on in the literature. 

getting it wrong has societal costs, as others will run the same studies because they think the line of work is real, or because they don't know there is evidence the effect is null. 


```{r}

# research efficacy. ie, discoveries per dollar / (false discoveries per dollar + missed discoveries per dollar)

# hare 
175/(25+70)

# tortoise
68/(4+11)

# relative research efficiency
(68/(4+11)) / (175/(25+70))

```





